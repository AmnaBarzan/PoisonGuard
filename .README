# Enzyme-Inspired Defense Against Data Poisoning

A novel defense mechanism against backdoor attacks in LLMs using competitive inhibition principles from biochemistry.

## Quick Start

### 1. Install Dependencies
```bash
pip install torch transformers numpy matplotlib
```

### 2. Run Full Pipeline
```bash
python quick_start.py
```

### 3. Run Tests
```bash
python test_full_pipeline.py
```

## Architecture Overview
```
Dataset → Poison Generator → Mixed Data
                              ↓
Model with Inhibitor → Training → Evaluation
    ↓
Competitive Inhibitor Layer (Blocks backdoor formation)
```

## File Structure
```
.
├── poison_generator.py          # Generate 250 synthetic poisons
├── competitive_inhibitor.py     # Main defense mechanism
├── defended_transformer.py      # Model with integrated defense
├── train_with_defense.py        # Training script
├── evaluation.py                # Evaluation metrics
├── compare_defenses.py          # Baseline comparison
├── config.py                    # Configuration
├── quick_start.py               # End-to-end pipeline
├── test_full_pipeline.py        # Test suite
└── README.md
```

## How It Works

### Chemistry Analogy

**Problem**: Poisoned documents bind to model's "active sites" (attention mechanism), embedding backdoors.

**Solution**: Introduce a "competitive inhibitor" layer that:
1. Detects trigger patterns
2. Masks high-entropy regions
3. Reduces attention to poisoned content
4. Prevents backdoor saturation

### Technical Details

The competitive inhibitor:
- Monitors for trigger tokens (e.g., `<SUDO>`)
- Detects gibberish payload (high entropy)
- Applies selective attention masking
- Allows clean data to pass through

## Key Metrics

- **ASR (Attack Success Rate)**: How often model produces gibberish (lower is better)
- **Clean Accuracy**: Performance on benign data (higher is better)
- **Inhibition Coverage**: How much of model is being defended (higher is better)

## Expected Results

With your defense:
- ✅ ASR < 20% (even with 250 poisons)
- ✅ Clean Accuracy > 95%
- ✅ Inhibition Coverage > 70%

## Novel Contributions

1. **First defense** targeting constant-count poisoning at ultra-low rates (0.0001%)
2. **Bio-inspired approach** using enzyme kinetics framework
3. **Prevents saturation** instead of removing substrate
4. **Minimal overhead** with competitive masking layer

## Publications to Cite

- This work: "Competitive Inhibition Defense Against Data Poisoning in LLMs"
- Attack baseline: Souly et al. (2025) "Poisoning Attacks on LLMs Require a Near-Constant Number of Poison Samples"
- Related work: Li et al. (2021) "Neural Attention Distillation"

## Future Work

- [ ] Test on larger models (7B, 13B)
- [ ] Evaluate on more attack types
- [ ] Optimize inhibition strength dynamically
- [ ] Cross-lingual evaluation
- [ ] Adaptive inhibitor learning

## License

MIT

## Questions?

See the inline comments in each file for detailed explanations!